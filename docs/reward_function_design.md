# 二阶倒立摆奖励函数设计

## 📋 目录
- [问题背景](#问题背景)
- [传统方法的问题](#传统方法的问题)
- [平滑高斯乘法奖励](#平滑高斯乘法奖励)
- [实验对比](#实验对比)
- [配置指南](#配置指南)

---

## 问题背景

二阶倒立摆是一个极具挑战性的控制任务：
- **两个摆杆**：第一摆杆连接小车，第二摆杆连接第一摆杆末端
- **耦合动力学**：两个摆杆相互影响，控制一个会影响另一个
- **多目标**：必须同时保持两个摆杆直立

### "弃车保帅"问题

在传统的**加法奖励**中：
```
Reward = Pole1_Reward + Pole2_Reward
```

智能体可能发现：
- 只保持 Pole1 直立（拿满分）
- 放弃 Pole2（损失一部分分）
- 总分依然不错

这导致智能体学会"弃车保帅"，只控制一个摆杆。

---

## 传统方法的问题

### 1. 加法奖励的问题

```python
# 加法奖励
reward = pole1_reward + pole2_reward
```

**问题**：Pole1 好可以弥补 Pole2 差

**示例**：
- Pole1 = 2°（奖励 +3.0）
- Pole2 = 30°（奖励 +1.67）
- 总奖励 = 4.67（还不错！）

智能体没有动力去救 Pole2。

### 2. 阶梯式乘法奖励的问题

虽然乘法解决了"弃车保帅"，但阶梯函数带来新问题：

```python
# 阶梯式乘法
if pole1_angle < 5:
    pole1_status = 1.0  # 死区！
elif pole1_angle < 15:
    pole1_status = 0.7  # 跳变！
```

**问题1：梯度死区**
- Pole1 在 0°-5° 之间，奖励永远是 1.0
- 智能体没有动力从 4° 优化到 1°
- 缺乏精益求精的反馈

**问题2：梯度不连续**
- 在 5°、15° 等切换点，奖励突变
- 导数不连续，学习不稳定
- PPO 难以获得稳定的梯度信号

**问题3：忽略角速度**
- 只看角度，不看速度
- 第二阶高速旋转经过垂直点也能拿满分
- "旋转刷分"问题

**问题4：不愿倾斜**
- Pole1 从 5° 变成 6°，奖励从 1.0 掉到 0.97
- 损失太大，智能体不敢尝试
- 不愿意倾斜去救 Pole2

---

## 平滑高斯乘法奖励

### 核心思想

使用**高斯函数**替代阶梯函数，引入**角速度惩罚**：

```python
# 角度状态（平滑高斯）
p1_angle_status = exp(-pole1_angle² / σ₁²)
p2_angle_status = exp(-pole2_angle² / σ₂²)

# 角速度状态
v1_status = exp(-pole1_velocity² / σ_v1²)
v2_status = exp(-pole2_velocity² / σ_v2²)

# 组合状态
pole1_total = p1_angle_status × (0.8 + 0.2 × v1_status)
pole2_total = p2_angle_status × (0.7 + 0.3 × v2_status)

# 总奖励（乘法）
reward = pole1_total × pole2_total × position_factor × 10
```

### 数学特性

**高斯函数** `f(x) = exp(-x²/σ²)` 的优势：

1. **处处可导**：在任何地方都有导数
2. **平滑过渡**：没有突变和跳跃
3. **物理直觉**：偏离越多，惩罚越重（二次增长）
4. **自然衰减**：远离目标时奖励自然趋近于 0

### 参数说明

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `angle1_sigma` | 0.10 | 第一摆杆角度容忍度（约18°时降到0.36） |
| `angle2_sigma` | 0.15 | 第二摆杆角度容忍度（稍微宽容） |
| `vel1_sigma` | 5.0 | 第一摆杆角速度容忍度 |
| `vel2_sigma` | 10.0 | 第二摆杆角速度容忍度 |
| `vel1_weight` | 0.2 | 第一摆杆速度权重（20%） |
| `vel2_weight` | 0.3 | 第二摆杆速度权重（30%，更重视静止） |

---

## 实验对比

### 测试场景

运行对比测试：
```bash
python compare_reward_functions.py
```

### 场景1：理想状态

```
Pole1 = 2°, Pole2 = 2°
- 加法奖励:       5.01
- 阶梯式乘法:    12.00
- 平滑高斯乘法:   9.80
```

### 场景2：弃车保帅

```
Pole1 = 2°, Pole2 = 30°（Pole2倒了）
- 加法奖励:       1.67  ← 还有正奖励！
- 阶梯式乘法:     3.50
- 平滑高斯乘法:   1.59  ← 严厉惩罚
```

### 场景3：旋转刷分

```
Pole1 = 2°, Pole2 = 5°, 但 Pole2 角速度 = 8
- 加法奖励:       5.01  ← 检测不到旋转
- 阶梯式乘法:    12.00  ← 检测不到旋转
- 平滑高斯乘法:   6.58  ← 惩罚旋转
```

### 场景4：梯度死区

```
Pole1 从 4° 优化到 1°，Pole2 保持 5°
- 加法奖励:       5.01 → 5.01 → 5.01 → 5.01  ← 没有反馈
- 阶梯式乘法:    12.00 → 12.00 → 12.00 → 12.00  ← 没有反馈
- 平滑高斯乘法:   9.05 → 9.25 → 9.39 → 9.48  ← 持续增长
```

### 场景5：协调控制

```
P1 适度倾斜能否换取 P2 的大幅改善？

配置1: P1=2°, P2=30° (P1完美，P2倒了)
- 平滑高斯: 1.59

配置2: P1=10°, P2=15° (P1倾斜10°，P2改善到15°)
- 平滑高斯: 4.67  ← 收益明显！

配置3: P1=15°, P2=10° (P1倾斜15°，P2改善到10°)
- 平滑高斯: 4.11  ← 依然有收益
```

**结论**：平滑高斯鼓励 P1 适度倾斜去救 P2。

---

## 配置指南

### 基础配置

在 `configs/double_pendulum_config.py` 中：

```python
'reward_config': {
    # 启用平滑高斯乘法奖励（推荐）
    'use_multiplicative': True,
    'use_smooth_gaussian': True,
    
    # 角度容忍度
    'angle1_sigma': 0.10,
    'angle2_sigma': 0.15,
    
    # 角速度容忍度
    'vel1_sigma': 5.0,
    'vel2_sigma': 10.0,
    
    # 速度权重
    'vel1_weight': 0.2,
    'vel2_weight': 0.3,
}
```

### 调参建议

**更严格的控制**（要求更高精度）：
```python
'angle1_sigma': 0.08,  # 降低容忍度
'angle2_sigma': 0.12,
'vel1_weight': 0.3,    # 增加速度权重
'vel2_weight': 0.4,
```

**更宽松的控制**（更容易学习）：
```python
'angle1_sigma': 0.15,  # 提高容忍度
'angle2_sigma': 0.20,
'vel1_weight': 0.1,    # 降低速度权重
'vel2_weight': 0.2,
```

**强调第二阶控制**：
```python
'angle2_sigma': 0.10,  # 对第二阶更严格
'vel2_weight': 0.4,    # 更重视第二阶的静止
```

### 对比测试

如果想对比不同奖励函数：

```python
# 使用加法奖励（旧版）
'use_multiplicative': False,

# 使用阶梯式乘法（中间版）
'use_multiplicative': True,
'use_smooth_gaussian': False,

# 使用平滑高斯乘法（推荐）
'use_multiplicative': True,
'use_smooth_gaussian': True,
```

---

## 训练建议

### 预期学习曲线

使用平滑高斯乘法奖励：

- **0-50次迭代**：探索阶段，奖励 0-3
- **50-150次迭代**：学会稳定 P1，奖励 3-5
- **150-250次迭代**：开始协调控制，奖励 5-8
- **250-400次迭代**：精细调优，奖励 8-10

### 关键改进点

相比旧版，你应该观察到：

✅ **第一阶不再"僵硬"**
- 会主动倾斜去帮助第二阶
- 不再死守 0° 不动

✅ **第二阶不再"旋转"**
- 角速度惩罚迫使它静止
- 不再高速旋转刷分

✅ **学习更稳定**
- 平滑梯度避免了突变
- 训练曲线更平滑

✅ **收敛更快**
- 每一点改进都有反馈
- 精益求精的动力

---

## 参考文献

### 理论基础

1. **乘法奖励**：避免"弃车保帅"问题
   - 核心思想：命运共同体，任一失败则总体失败

2. **高斯函数**：平滑的奖励塑形
   - 处处可导，提供稳定梯度
   - 物理直觉：偏离越多，惩罚越重

3. **角速度惩罚**：区分"静止的直立"和"旋转经过直立"
   - 避免刷分行为
   - 鼓励真正的稳定控制

### 相关工作

- Reward Shaping in Reinforcement Learning
- Continuous Control with Deep Reinforcement Learning
- Multiplicative Reward Functions for Multi-Objective Tasks

---

## 总结

平滑高斯乘法奖励通过以下创新解决了二阶倒立摆的控制难题：

1. ✅ **消除梯度死区** - 处处可导的高斯函数
2. ✅ **杀死旋转刷分** - 角速度惩罚
3. ✅ **鼓励协调控制** - 平滑的乘法关系
4. ✅ **数学优雅** - 自然、连续、可导

这是一个从实践中总结出来的创新设计，为复杂控制任务的奖励函数设计提供了新的思路。

---

**作者**: RLGym2D Team  
**日期**: 2026-01-03  
**版本**: 1.1.0
