"""
è®­ç»ƒæœºæ¢°è‡‚æ¨¡å‹
"""
import os
import warnings
import json
from datetime import datetime
from collections import defaultdict

# æŠ‘åˆ¶pygameçš„deprecationè­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='pygame.pkgdata')

from simple_arm_env import SimpleArmEnv
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.callbacks import BaseCallback, CallbackList
import numpy as np
from config import get_config, print_config


class RewardComponentLogger(BaseCallback):
    """è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±ç»„ä»¶"""
    
    def __init__(self, log_freq=100, verbose=0):
        super().__init__(verbose)
        self.log_freq = log_freq
        self.episode_rewards = defaultdict(list)
        self.episode_count = 0
        self.current_episode_rewards = defaultdict(float)
    
    def _on_step(self):
        # è·å–å½“å‰ç¯å¢ƒçš„info
        if len(self.locals.get('infos', [])) > 0:
            info = self.locals['infos'][0]
            
            # ç´¯ç§¯å½“å‰episodeçš„å¥–åŠ±ç»„ä»¶
            if 'reward_components' in info:
                for comp_name, comp_value in info['reward_components'].items():
                    self.current_episode_rewards[comp_name] += float(comp_value)
            
            # æ£€æŸ¥episodeæ˜¯å¦ç»“æŸ
            dones = self.locals.get('dones', [False])
            if dones[0]:
                self.episode_count += 1
                
                # è®°å½•è¿™ä¸ªepisodeçš„å¥–åŠ±ç»„ä»¶
                for comp_name, comp_value in self.current_episode_rewards.items():
                    self.episode_rewards[comp_name].append(comp_value)
                
                # æ¯log_freqä¸ªepisodeè®°å½•ä¸€æ¬¡å¹³å‡å€¼
                if self.episode_count % self.log_freq == 0:
                    # è®¡ç®—å¹³å‡å€¼
                    avg_rewards = {}
                    total_reward = 0
                    for comp_name, values in self.episode_rewards.items():
                        if len(values) > 0:
                            avg_value = np.mean(values[-self.log_freq:])
                            avg_rewards[comp_name] = avg_value
                            total_reward += avg_value
                    
                    # è®¡ç®—è´¡çŒ®ç™¾åˆ†æ¯”
                    contributions = {}
                    if total_reward != 0:
                        for comp_name, avg_value in avg_rewards.items():
                            contributions[comp_name] = (avg_value / total_reward) * 100
                    
                    # è®°å½•åˆ°TensorBoard
                    for comp_name, avg_value in avg_rewards.items():
                        self.logger.record(f'train/reward_component/{comp_name}', avg_value)
                        if comp_name in contributions:
                            self.logger.record(f'train/reward_contribution/{comp_name}_percent', contributions[comp_name])
                    
                    # è®°å½•åˆ°W&B
                    try:
                        import wandb
                        if wandb.run is not None:
                            log_dict = {}
                            for comp_name, avg_value in avg_rewards.items():
                                log_dict[f'train/reward_component/{comp_name}'] = avg_value
                                if comp_name in contributions:
                                    log_dict[f'train/reward_contribution/{comp_name}_percent'] = contributions[comp_name]
                            wandb.log(log_dict, step=self.num_timesteps)
                    except:
                        pass
                
                # é‡ç½®å½“å‰episodeçš„ç´¯ç§¯
                self.current_episode_rewards = defaultdict(float)
        
        return True


class PerformanceCallback(BaseCallback):
    """è‡ªå®šä¹‰å›è°ƒï¼šè®°å½•è®­ç»ƒæ€§èƒ½æŒ‡æ ‡"""
    
    def __init__(self, eval_env, eval_freq=5000, n_eval_episodes=10, verbose=1, log_reward_components=True, best_model_save_path=None, checkpoint_dir=None, checkpoint_freq=1):
        super().__init__(verbose)
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        self.evaluations_results = []
        self.evaluations_timesteps = []
        self.best_mean_reward = -np.inf
        self.log_reward_components = log_reward_components
        self.best_model_save_path = best_model_save_path  # æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„
        self.checkpoint_dir = checkpoint_dir  # checkpointä¿å­˜ç›®å½•
        self.checkpoint_freq = checkpoint_freq  # æ¯Næ¬¡è¯„ä¼°ä¿å­˜ä¸€æ¬¡
        self.eval_count = 0  # è¯„ä¼°è®¡æ•°å™¨
        self.checkpoint_count = 0  # checkpointè®¡æ•°å™¨
    
    def _on_step(self):
        # ä½¿ç”¨ num_timesteps è€Œä¸æ˜¯ n_callsï¼Œç¡®ä¿æŒ‰ç¯å¢ƒæ­¥æ•°è¯„ä¼°
        if self.num_timesteps % self.eval_freq == 0:
            # è¯„ä¼°å½“å‰ç­–ç•¥
            episode_rewards = []
            episode_lengths = []
            success_count = 0
            reward_components_sum = {}
            
            for _ in range(self.n_eval_episodes):
                obs, info = self.eval_env.reset()
                done = False
                episode_reward = 0
                episode_length = 0
                episode_components = {}
                
                while not done:
                    action, _ = self.model.predict(obs, deterministic=True)
                    obs, reward, terminated, truncated, info = self.eval_env.step(action)
                    episode_reward += reward
                    episode_length += 1
                    done = terminated or truncated
                    
                    # ç´¯ç§¯å¥–åŠ±ç»„ä»¶
                    if self.log_reward_components and 'reward_components' in info:
                        for comp_name, comp_value in info['reward_components'].items():
                            if comp_name not in episode_components:
                                episode_components[comp_name] = 0
                            episode_components[comp_name] += float(comp_value)
                    
                    if terminated:
                        success_count += 1
                
                episode_rewards.append(episode_reward)
                episode_lengths.append(episode_length)
                
                # ç´¯ç§¯æ‰€æœ‰å›åˆçš„ç»„ä»¶
                for comp_name, comp_value in episode_components.items():
                    if comp_name not in reward_components_sum:
                        reward_components_sum[comp_name] = []
                    reward_components_sum[comp_name].append(comp_value)
            
            mean_reward = np.mean(episode_rewards)
            std_reward = np.std(episode_rewards)
            mean_length = np.mean(episode_lengths)
            success_rate = success_count / self.n_eval_episodes * 100
            
            self.evaluations_results.append(mean_reward)
            self.evaluations_timesteps.append(self.num_timesteps)
            
            # è®°å½•åˆ°W&B
            try:
                import wandb
                if wandb.run is not None:
                    log_dict = {
                        'eval/mean_reward': mean_reward,
                        'eval/std_reward': std_reward,
                        'eval/mean_length': mean_length,
                        'eval/success_rate': success_rate,
                        'eval/best_mean_reward': self.best_mean_reward,
                    }
                    
                    # æ·»åŠ å¥–åŠ±ç»„ä»¶
                    if self.log_reward_components and reward_components_sum:
                        for comp_name, comp_values in reward_components_sum.items():
                            log_dict[f'eval/reward_component/{comp_name}'] = np.mean(comp_values)
                    
                    wandb.log(log_dict, step=self.num_timesteps)
            except:
                pass
            
            if self.verbose > 0:
                print(f"\n{'='*60}")
                print(f"è¯„ä¼° @ {self.num_timesteps} æ­¥:")
                print(f"  å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
                print(f"  å¹³å‡é•¿åº¦: {mean_length:.1f}")
                print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
                print(f"{'='*60}\n")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if mean_reward > self.best_mean_reward:
                self.best_mean_reward = mean_reward
                if self.verbose > 0:
                    print(f"ğŸŒŸ æ–°çš„æœ€ä½³æ¨¡å‹ï¼å¥–åŠ±: {mean_reward:.2f}")
                
                if self.best_model_save_path is not None:
                    best_model_path = os.path.join(self.best_model_save_path, "best_model")
                    self.model.save(best_model_path)
                    if self.verbose > 0:
                        print(f"   å·²ä¿å­˜åˆ°: {best_model_path}.zip")
                
                # è®°å½•æœ€ä½³æ¨¡å‹åˆ°W&B
                try:
                    import wandb
                    if wandb.run is not None:
                        wandb.run.summary['best_mean_reward'] = mean_reward
                        wandb.run.summary['best_success_rate'] = success_rate
                        wandb.run.summary['best_timestep'] = self.num_timesteps
                except:
                    pass
            
            # æ¯æ¬¡è¯„ä¼°éƒ½ä¿å­˜checkpoint
            self.eval_count += 1
            if self.checkpoint_dir is not None and self.eval_count % self.checkpoint_freq == 0:
                # è®¡ç®—å½“å‰è¿­ä»£æ¬¡æ•°ï¼ˆè¿‘ä¼¼ï¼‰
                iteration = self.num_timesteps // (self.model.n_steps * self.model.n_envs)
                
                # åˆ›å»ºcheckpointæ–‡ä»¶åï¼šåŒ…å«è¿­ä»£æ¬¡æ•°ã€å¥–åŠ±ã€æˆåŠŸç‡
                self.checkpoint_count += 1
                checkpoint_name = f"ckpt_iter_{iteration:03d}_reward_{mean_reward:+.1f}_success_{success_rate:.2f}.zip"
                checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)
                
                # ä¿å­˜checkpoint
                self.model.save(checkpoint_path)
                
                if self.verbose > 0:
                    print(f"ğŸ’¾ ä¿å­˜checkpoint #{self.checkpoint_count}: {checkpoint_name}")
        
        return True


def make_env(rank, seed=0, env_config=None, reward_config=None):
    """åˆ›å»ºç¯å¢ƒçš„å·¥å‚å‡½æ•°"""
    def _init():
        env = SimpleArmEnv(render_mode=None, config=env_config, reward_config=reward_config)
        env.reset(seed=seed + rank)
        return env
    return _init


def train(config=None, preset=None, n_envs=None):
    """è®­ç»ƒæ¨¡å‹"""
    if config is None:
        config = get_config(preset)
    
    train_cfg = config['training']['parallel']
    ppo_cfg = config['ppo']
    
    if n_envs is None:
        n_envs = train_cfg['n_envs']
    
    # è®¡ç®—total_timestepsï¼ˆæ”¯æŒä¸¤ç§é…ç½®æ–¹å¼ï¼‰
    if 'n_iterations' in train_cfg:
        # æ–¹å¼1ï¼šç›´æ¥æŒ‡å®šè¿­ä»£æ¬¡æ•°
        n_iterations = train_cfg['n_iterations']
        total_timesteps = n_iterations * ppo_cfg['n_steps'] * n_envs
        print(f"\né…ç½®: {n_iterations}æ¬¡è¿­ä»£ Ã— {ppo_cfg['n_steps']}æ­¥ Ã— {n_envs}ç¯å¢ƒ = {total_timesteps:,}æ­¥")
    else:
        # æ–¹å¼2ï¼šæŒ‡å®šæ€»æ­¥æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
        total_timesteps = train_cfg.get('total_timesteps', 1000000)
        n_iterations = total_timesteps // (ppo_cfg['n_steps'] * n_envs)
        print(f"\né…ç½®: {total_timesteps:,}æ­¥ â‰ˆ {n_iterations}æ¬¡è¿­ä»£")
    
    train_cfg['total_timesteps'] = total_timesteps  # æ›´æ–°é…ç½®
    
    # éªŒè¯ç¯å¢ƒæ•°æ˜¯å¦åˆç†
    import multiprocessing
    cpu_count = multiprocessing.cpu_count()
    max_recommended = cpu_count * 4
    
    if n_envs > max_recommended:
        print(f"\n{'='*70}")
        print(f"âš ï¸  è­¦å‘Šï¼šå¹¶è¡Œç¯å¢ƒæ•°è¿‡å¤šï¼")
        print(f"{'='*70}")
        print(f"  è¯·æ±‚çš„ç¯å¢ƒæ•°: {n_envs}")
        print(f"  CPUæ ¸å¿ƒæ•°: {cpu_count}")
        print(f"  æ¨èç¯å¢ƒæ•°: {cpu_count * 2}")
        print(f"  æœ€å¤§æ¨è: {max_recommended}")
        print(f"\n  è¿‡å¤šçš„ç¯å¢ƒä¼šå¯¼è‡´:")
        print(f"    - å†…å­˜è€—å°½")
        print(f"    - ç³»ç»Ÿå¡æ­»")
        print(f"    - è®­ç»ƒå´©æºƒ")
        print(f"{'='*70}")
        
        response = input(f"\næ˜¯å¦ç»§ç»­ï¼Ÿ(y/nï¼Œæ¨èè¾“å…¥ n): ").strip().lower()
        if response != 'y':
            print(f"\nå»ºè®®ä½¿ç”¨ {cpu_count * 2} ä¸ªç¯å¢ƒ")
            return None, None
        else:
            print("\nâš ï¸  ç»§ç»­ä½¿ç”¨è¿‡å¤šç¯å¢ƒï¼Œé£é™©è‡ªè´Ÿ...")
    
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¿è¡Œç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    project_name = "2d_arm_rl"
    run_name = f"{project_name}_{timestamp}"
    run_dir = os.path.join("runs", run_name)
    os.makedirs(run_dir, exist_ok=True)
    
    print("=" * 70)
    print(f"è®­ç»ƒ ({n_envs} ä¸ªå¹¶è¡Œç¯å¢ƒ)")
    print("=" * 70)
    print(f"è¿è¡Œåç§°: {run_name}")
    print(f"ä¿å­˜ç›®å½•: {run_dir}/")
    print("=" * 70)
    
    # æ£€æµ‹å¹¶æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
    import torch
    device = config['device']['device']
    if device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print("\n" + "=" * 60)
    print("è®­ç»ƒè®¾å¤‡ä¿¡æ¯")
    print("=" * 60)
    if device == 'cuda' and torch.cuda.is_available():
        print(f"âœ“ ä½¿ç”¨ GPU è®­ç»ƒ")
        print(f"  GPUå‹å·: {torch.cuda.get_device_name(0)}")
        print(f"  æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  PyTorchè®¾å¤‡: {device}")
        print(f"  å¹¶è¡Œç¯å¢ƒæ•°: {n_envs}")
        print(f"  é¢„è®¡åŠ é€Ÿ: 5-10å€ï¼ˆç›¸æ¯”CPUï¼‰")
    else:
        print(f"âœ“ ä½¿ç”¨ CPU è®­ç»ƒ")
        print(f"  å¹¶è¡Œç¯å¢ƒæ•°: {n_envs}")
        print(f"  æç¤º: å®‰è£…CUDAç‰ˆPyTorchå¯è·å¾—5-10å€åŠ é€Ÿ")
    print("=" * 60 + "\n")
    
    # åˆ›å»ºå¹¶è¡Œè®­ç»ƒç¯å¢ƒ
    env = SubprocVecEnv([make_env(i, env_config=config['env'], reward_config=config['reward']) for i in range(n_envs)])
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    eval_env = SimpleArmEnv(render_mode=None, config=config['env'], reward_config=config['reward'])
    
    # ä¿å­˜é…ç½®åˆ°è¿è¡Œç›®å½•
    config_save = {
        'run_name': run_name,
        'timestamp': timestamp,
        'n_envs': n_envs,
        'env_config': config['env'],
        'reward_config': config['reward'],
        'ppo_config': {k: str(v) if not isinstance(v, (int, float, bool, type(None))) else v 
                       for k, v in ppo_cfg.items()},
        'training_config': train_cfg,
        'network_config': {
            'net_arch': config['network']['policy_kwargs']['net_arch']
        }
    }
    with open(os.path.join(run_dir, 'config.json'), 'w', encoding='utf-8') as f:
        json.dump(config_save, f, indent=2, ensure_ascii=False)
    
    # åˆå§‹åŒ– W&Bï¼ˆå¦‚æœå¯ç”¨ï¼‰
    wandb_run = None
    if config.get('wandb', {}).get('enabled', False):
        try:
            import wandb
            wandb_run = wandb.init(
                project=config['wandb']['project'],
                entity=config['wandb']['entity'],
                name=config['wandb']['name'] or run_name,
                tags=config['wandb']['tags'],
                notes=config['wandb']['notes'],
                config=config_save,
                sync_tensorboard=config['wandb']['sync_tensorboard'],
            )
            print("âœ“ W&B æ—¥å¿—å·²å¯ç”¨")
            print(f"  é¡¹ç›®: {config['wandb']['project']}")
            print(f"  è¿è¡Œ: {wandb_run.name}")
            print(f"  é“¾æ¥: {wandb_run.url}\n")
        except ImportError:
            print("âš  W&B æœªå®‰è£…ï¼Œè·³è¿‡")
            print("  å®‰è£…: pip install wandb\n")
        except Exception as e:
            print(f"âš  W&B åˆå§‹åŒ–å¤±è´¥: {e}\n")
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆTensorBoardæ—¥å¿—ä¿å­˜åˆ°è¿è¡Œç›®å½•ï¼‰
    tensorboard_log = os.path.join(run_dir, "tensorboard")
    model = PPO(
        ppo_cfg['policy'],
        env,
        verbose=ppo_cfg['verbose'],
        learning_rate=ppo_cfg['learning_rate'],
        n_steps=ppo_cfg['n_steps'],
        batch_size=ppo_cfg['batch_size'],
        n_epochs=ppo_cfg['n_epochs'],
        gamma=ppo_cfg['gamma'],
        gae_lambda=ppo_cfg['gae_lambda'],
        clip_range=ppo_cfg['clip_range'],
        ent_coef=ppo_cfg['ent_coef'],
        vf_coef=ppo_cfg['vf_coef'],
        max_grad_norm=ppo_cfg['max_grad_norm'],
        policy_kwargs=config['network']['policy_kwargs'],
        tensorboard_log=tensorboard_log,
        device=device
    )
    
    # åˆ›å»ºæœ€ä½³æ¨¡å‹ä¿å­˜ç›®å½•
    best_model_dir = os.path.join(run_dir, "best_model")
    os.makedirs(best_model_dir, exist_ok=True)
    
    # åˆ›å»ºcheckpointä¿å­˜ç›®å½•
    checkpoint_dir = os.path.join(run_dir, "checkpoints")
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # åˆ›å»ºå›è°ƒåˆ—è¡¨
    reward_logger = RewardComponentLogger(log_freq=100, verbose=0)
    performance_callback = PerformanceCallback(
        eval_env=eval_env,
        eval_freq=train_cfg['eval_freq'],
        n_eval_episodes=train_cfg['n_eval_episodes'],
        verbose=1,
        best_model_save_path=best_model_dir,
        checkpoint_dir=checkpoint_dir,
        checkpoint_freq=train_cfg.get('checkpoint_freq', 1)
    )
    
    # ç»„åˆå›è°ƒ
    callbacks = CallbackList([reward_logger, performance_callback])
    
    # è®­ç»ƒ
    print(f"\nå¼€å§‹è®­ç»ƒ...")
    print(f"PPOè¿­ä»£æ¬¡æ•°: {n_iterations}")
    print(f"æ€»ç¯å¢ƒæ­¥æ•°: {train_cfg['total_timesteps']:,}")
    print(f"æ¯æ¬¡è¿­ä»£: {ppo_cfg['n_steps']} Ã— {n_envs} = {ppo_cfg['n_steps'] * n_envs:,}æ­¥")
    print(f"è¯„ä¼°é¢‘ç‡: æ¯{train_cfg['eval_freq']}æ­¥")
    print(f"å¥–åŠ±ç»„ä»¶è®°å½•: æ¯100ä¸ªepisode\n")
    model.learn(total_timesteps=train_cfg['total_timesteps'], progress_bar=True, callback=callbacks)
    
    # ä¿å­˜æ¨¡å‹åˆ°è¿è¡Œç›®å½•
    model_path = os.path.join(run_dir, "model")
    model.save(model_path)
    
    # ä¿å­˜è®­ç»ƒæ‘˜è¦
    best_model_path = os.path.join(best_model_dir, "best_model")
    summary = {
        'run_name': run_name,
        'timestamp': timestamp,
        'total_timesteps': train_cfg['total_timesteps'],
        'n_envs': n_envs,
        'device': device,
        'final_model_path': f"{model_path}.zip",
        'best_model_path': f"{best_model_path}.zip",
        'tensorboard_log': tensorboard_log,
        'best_reward': performance_callback.best_mean_reward if hasattr(performance_callback, 'best_mean_reward') else None,
    }
    
    with open(os.path.join(run_dir, 'summary.json'), 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»º README
    readme_content = f"""# è®­ç»ƒè¿è¡Œ: {run_name}

## åŸºæœ¬ä¿¡æ¯
- **è¿è¡Œæ—¶é—´**: {timestamp}
- **è®­ç»ƒæ­¥æ•°**: {train_cfg['total_timesteps']:,}
- **å¹¶è¡Œç¯å¢ƒ**: {n_envs}
- **è®¾å¤‡**: {device}

## æ–‡ä»¶è¯´æ˜
- `model.zip` - æœ€ç»ˆæ¨¡å‹ï¼ˆè®­ç»ƒç»“æŸæ—¶ï¼‰
- `best_model/best_model.zip` - æœ€ä½³æ¨¡å‹ï¼ˆè¯„ä¼°å¥–åŠ±æœ€é«˜ï¼‰â­
- `checkpoints/` - æ¯æ¬¡è¯„ä¼°éƒ½ä¿å­˜checkpoint
- `config.json` - å®Œæ•´é…ç½®
- `summary.json` - è®­ç»ƒæ‘˜è¦
- `tensorboard/` - TensorBoard æ—¥å¿—

## å¦‚ä½•ä½¿ç”¨

### æ¼”ç¤ºæœ€ä½³æ¨¡å‹ï¼ˆæ¨èï¼‰
```bash
python play.py --model {best_model_path}
```

### æ¼”ç¤ºæœ€ç»ˆæ¨¡å‹
```bash
python play.py --model {model_path}
```

### æ¼”ç¤ºæŸä¸ªcheckpoint
```bash
python play.py --model {os.path.join(run_dir, 'checkpoints', 'ckpt_iter_XXX_reward_YYY_success_ZZZ')}
```

### è¯„ä¼°æ¨¡å‹
```bash
python evaluate_performance.py
# è¾“å…¥è·¯å¾„: {model_path}
```

### æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
```bash
tensorboard --logdir={tensorboard_log}
```
"""
    
    with open(os.path.join(run_dir, 'README.md'), 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print("\n" + "=" * 70)
    print("è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)
    print(f"è¿è¡Œç›®å½•: {run_dir}/")
    print(f"  â”œâ”€â”€ model.zip                    (æœ€ç»ˆæ¨¡å‹)")
    print(f"  â”œâ”€â”€ best_model/best_model.zip    (æœ€ä½³æ¨¡å‹) â­")
    print(f"  â”œâ”€â”€ checkpoints/                 (å®šæœŸcheckpoint)")
    print(f"  â”œâ”€â”€ config.json                  (é…ç½®)")
    print(f"  â”œâ”€â”€ summary.json                 (æ‘˜è¦)")
    print(f"  â”œâ”€â”€ README.md                    (è¯´æ˜)")
    print(f"  â””â”€â”€ tensorboard/                 (æ—¥å¿—)")
    print("=" * 70)
    print(f"\næœ€ä½³è¯„ä¼°å¥–åŠ±: {performance_callback.best_mean_reward:.2f}")
    print(f"æ¨èä½¿ç”¨æœ€ä½³æ¨¡å‹: {best_model_path}.zip")
    print(f"ä¿å­˜çš„checkpointæ•°é‡: {performance_callback.checkpoint_count}")
    print("=" * 70)
    
    # å…³é—­ W&B
    if wandb_run is not None:
        try:
            import wandb
            wandb.finish()
            print("âœ“ W&B æ—¥å¿—å·²ä¸Šä¼ ")
        except:
            pass
    
    env.close()
    eval_env.close()
    return model, run_dir


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='è®­ç»ƒ2Dæœºæ¢°è‡‚æ¨¡å‹')
    parser.add_argument('--envs', type=int, default=None,
                       help='å¹¶è¡Œç¯å¢ƒæ•°é‡ï¼ˆè¦†ç›–config.pyä¸­çš„è®¾ç½®ï¼‰')
    parser.add_argument('--play', action='store_true',
                       help='è®­ç»ƒåç«‹å³æ¼”ç¤º')
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("2Dæœºæ¢°è‡‚å¼ºåŒ–å­¦ä¹ è®­ç»ƒ")
    print("=" * 70)
    
    # ç›´æ¥ä½¿ç”¨config.pyä¸­çš„é…ç½®
    config = get_config()
    print("\nâœ“ ä½¿ç”¨ config.py ä¸­çš„é…ç½®")
    print_config(config)
    
    # ç¡®å®šç¯å¢ƒæ•°
    if args.envs is None:
        args.envs = config['training']['parallel']['n_envs']
    else:
        print(f"\nâš ï¸  å‘½ä»¤è¡Œå‚æ•°è¦†ç›–: ä½¿ç”¨ {args.envs} ä¸ªå¹¶è¡Œç¯å¢ƒ")
    
    # å¼€å§‹è®­ç»ƒ
    result = train(config=config, n_envs=args.envs)
    
    if result is None or result[0] is None:
        print("\nè®­ç»ƒå·²å–æ¶ˆæˆ–å¤±è´¥")
    else:
        model, run_dir = result
        
        # è¯¢é—®æ˜¯å¦æ¼”ç¤º
        if args.play:
            play = 'y'
        else:
            play = input("\næ˜¯å¦æ¼”ç¤ºè®­ç»ƒç»“æœï¼Ÿ(y/n): ").strip().lower()
        
        if play == 'y':
            # ä¼˜å…ˆä½¿ç”¨æœ€ä½³æ¨¡å‹
            best_model_path = os.path.join(run_dir, "best_model", "best_model")
            if os.path.exists(f"{best_model_path}.zip"):
                demo_model = best_model_path
                print(f"\nå¯åŠ¨æ¼”ç¤ºï¼ˆä½¿ç”¨æœ€ä½³æ¨¡å‹ï¼‰...")
            else:
                demo_model = os.path.join(run_dir, "model")
                print(f"\nå¯åŠ¨æ¼”ç¤ºï¼ˆä½¿ç”¨æœ€ç»ˆæ¨¡å‹ï¼‰...")
            os.system(f"python play.py --model {demo_model} --episodes 3")
