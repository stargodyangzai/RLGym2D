"""
å¤šä»»åŠ¡ç»Ÿä¸€è®­ç»ƒè„šæœ¬

æ”¯æŒçš„ä»»åŠ¡ï¼š
- arm: 2Dæœºæ¢°è‡‚åˆ°è¾¾ç›®æ ‡
- walker: 2Dç«æŸ´äººè¡Œèµ°
- cartpole: å€’ç«‹æ‘†å¹³è¡¡
- double_pendulum: äºŒé˜¶å€’ç«‹æ‘†å¹³è¡¡

ä½¿ç”¨æ–¹å¼ï¼š
    # æ–°è®­ç»ƒ
    python train.py --task arm --envs 32
    python train.py --task walker --envs 16
    python train.py --task cartpole --envs 8
    python train.py --task double_pendulum --envs 16
    
    # ç»§ç»­è®­ç»ƒ
    python train.py --task double_pendulum --continue-from runs/xxx/best_model/best_model.zip
    python train.py --task double_pendulum --continue-from runs/xxx/best_model/best_model.zip --continue-iterations 200
    
    # è®­ç»ƒåç«‹å³æ¼”ç¤º
    python train.py --task double_pendulum --envs 16 --play
"""
import os
import sys
import argparse
import warnings
import json
from datetime import datetime
from collections import defaultdict

# è®¾ç½®ç¯å¢ƒå˜é‡æŠ‘åˆ¶è­¦å‘Šï¼ˆå¿…é¡»åœ¨å¯¼å…¥å…¶ä»–æ¨¡å—å‰ï¼‰
os.environ['PYTHONWARNINGS'] = 'ignore'

from envs import make_env
from configs import TASK_CONFIGS
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.callbacks import BaseCallback, CallbackList
import numpy as np
import torch

# æŠ‘åˆ¶æ‰€æœ‰è­¦å‘Š
warnings.filterwarnings('ignore')
warnings.simplefilter('ignore')


class RewardComponentLogger(BaseCallback):
    """è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±ç»„ä»¶"""
    
    def __init__(self, log_freq=100, verbose=0):
        super().__init__(verbose)
        self.log_freq = log_freq
        self.episode_rewards = defaultdict(list)
        self.episode_count = 0
        self.current_episode_rewards = defaultdict(float)
    
    def _on_step(self):
        # è·å–å½“å‰ç¯å¢ƒçš„info
        if len(self.locals.get('infos', [])) > 0:
            info = self.locals['infos'][0]
            
            # ç´¯ç§¯å½“å‰episodeçš„å¥–åŠ±ç»„ä»¶
            if 'reward_components' in info:
                for comp_name, comp_value in info['reward_components'].items():
                    self.current_episode_rewards[comp_name] += float(comp_value)
            
            # æ£€æŸ¥episodeæ˜¯å¦ç»“æŸ
            dones = self.locals.get('dones', [False])
            if dones[0]:
                self.episode_count += 1
                
                # è®°å½•è¿™ä¸ªepisodeçš„å¥–åŠ±ç»„ä»¶
                for comp_name, comp_value in self.current_episode_rewards.items():
                    self.episode_rewards[comp_name].append(comp_value)
                
                # æ¯log_freqä¸ªepisodeè®°å½•ä¸€æ¬¡å¹³å‡å€¼
                if self.episode_count % self.log_freq == 0:
                    # è®¡ç®—å¹³å‡å€¼
                    avg_rewards = {}
                    total_reward = 0
                    for comp_name, values in self.episode_rewards.items():
                        if len(values) > 0:
                            avg_value = np.mean(values[-self.log_freq:])
                            avg_rewards[comp_name] = avg_value
                            total_reward += avg_value
                    
                    # è®¡ç®—è´¡çŒ®ç™¾åˆ†æ¯”
                    contributions = {}
                    if total_reward != 0:
                        for comp_name, avg_value in avg_rewards.items():
                            contributions[comp_name] = (avg_value / total_reward) * 100
                    
                    # è®°å½•åˆ°TensorBoard
                    for comp_name, avg_value in avg_rewards.items():
                        self.logger.record(f'train/reward_component/{comp_name}', avg_value)
                        if comp_name in contributions:
                            self.logger.record(f'train/reward_contribution/{comp_name}_percent', contributions[comp_name])
                    
                    # è®°å½•åˆ°W&B
                    try:
                        import wandb
                        if wandb.run is not None:
                            log_dict = {}
                            for comp_name, avg_value in avg_rewards.items():
                                log_dict[f'train/reward_component/{comp_name}'] = avg_value
                                if comp_name in contributions:
                                    log_dict[f'train/reward_contribution/{comp_name}_percent'] = contributions[comp_name]
                            wandb.log(log_dict, step=self.num_timesteps)
                    except:
                        pass
                
                # é‡ç½®å½“å‰episodeçš„ç´¯ç§¯
                self.current_episode_rewards = defaultdict(float)
        
        return True


class PerformanceCallback(BaseCallback):
    """è‡ªå®šä¹‰å›è°ƒï¼šè®°å½•è®­ç»ƒæ€§èƒ½æŒ‡æ ‡"""
    
    def __init__(self, eval_env, eval_freq=5000, n_eval_episodes=10, verbose=1, 
                 log_reward_components=True, best_model_save_path=None, 
                 checkpoint_dir=None, checkpoint_freq=10):
        super().__init__(verbose)
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        self.evaluations_results = []
        self.evaluations_timesteps = []
        self.best_mean_reward = -np.inf
        self.log_reward_components = log_reward_components
        self.best_model_save_path = best_model_save_path
        self.checkpoint_dir = checkpoint_dir
        self.checkpoint_freq = checkpoint_freq
        self.eval_count = 0
        self.checkpoint_count = 0
    
    def _on_step(self):
        # ä½¿ç”¨ num_timesteps è€Œä¸æ˜¯ n_callsï¼Œç¡®ä¿æŒ‰ç¯å¢ƒæ­¥æ•°è¯„ä¼°
        if self.num_timesteps % self.eval_freq == 0:
            # è¯„ä¼°å½“å‰ç­–ç•¥
            episode_rewards = []
            episode_lengths = []
            success_count = 0
            reward_components_sum = {}
            
            for _ in range(self.n_eval_episodes):
                obs, info = self.eval_env.reset()
                done = False
                episode_reward = 0
                episode_length = 0
                episode_components = {}
                
                while not done:
                    action, _ = self.model.predict(obs, deterministic=True)
                    obs, reward, terminated, truncated, info = self.eval_env.step(action)
                    episode_reward += reward
                    episode_length += 1
                    done = terminated or truncated
                    
                    # ç´¯ç§¯å¥–åŠ±ç»„ä»¶
                    if self.log_reward_components and 'reward_components' in info:
                        for comp_name, comp_value in info['reward_components'].items():
                            if comp_name not in episode_components:
                                episode_components[comp_name] = 0
                            episode_components[comp_name] += float(comp_value)
                    
                    if terminated:
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„æˆåŠŸæ ‡å¿—
                        if 'success' in info and info['success']:
                            success_count += 1
                
                episode_rewards.append(episode_reward)
                episode_lengths.append(episode_length)
                
                # ç´¯ç§¯æ‰€æœ‰å›åˆçš„ç»„ä»¶
                for comp_name, comp_value in episode_components.items():
                    if comp_name not in reward_components_sum:
                        reward_components_sum[comp_name] = []
                    reward_components_sum[comp_name].append(comp_value)
            
            mean_reward = np.mean(episode_rewards)
            std_reward = np.std(episode_rewards)
            mean_length = np.mean(episode_lengths)
            success_rate = success_count / self.n_eval_episodes * 100
            
            self.evaluations_results.append(mean_reward)
            self.evaluations_timesteps.append(self.num_timesteps)
            
            # è®°å½•åˆ°W&B
            try:
                import wandb
                if wandb.run is not None:
                    log_dict = {
                        'eval/mean_reward': mean_reward,
                        'eval/std_reward': std_reward,
                        'eval/mean_length': mean_length,
                        'eval/success_rate': success_rate,
                        'eval/best_mean_reward': self.best_mean_reward,
                    }
                    
                    # æ·»åŠ å¥–åŠ±ç»„ä»¶
                    if self.log_reward_components and reward_components_sum:
                        for comp_name, comp_values in reward_components_sum.items():
                            log_dict[f'eval/reward_component/{comp_name}'] = np.mean(comp_values)
                    
                    wandb.log(log_dict, step=self.num_timesteps)
            except:
                pass
            
            if self.verbose > 0:
                print(f"\n{'='*60}")
                print(f"è¯„ä¼° @ {self.num_timesteps} æ­¥:")
                print(f"  å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
                print(f"  å¹³å‡é•¿åº¦: {mean_length:.1f}")
                # åªåœ¨æœ‰æˆåŠŸäº‹ä»¶æ—¶æ˜¾ç¤ºæˆåŠŸç‡
                if success_count > 0:
                    print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
                print(f"{'='*60}\n")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if mean_reward > self.best_mean_reward:
                self.best_mean_reward = mean_reward
                if self.verbose > 0:
                    print(f"ğŸŒŸ æ–°çš„æœ€ä½³æ¨¡å‹ï¼å¥–åŠ±: {mean_reward:.2f}")
                
                if self.best_model_save_path is not None:
                    best_model_path = os.path.join(self.best_model_save_path, "best_model")
                    self.model.save(best_model_path)
                    if self.verbose > 0:
                        print(f"   å·²ä¿å­˜åˆ°: {best_model_path}.zip")
                
                # è®°å½•æœ€ä½³æ¨¡å‹åˆ°W&B
                try:
                    import wandb
                    if wandb.run is not None:
                        wandb.run.summary['best_mean_reward'] = float(mean_reward)  # ç¡®ä¿æ˜¯Python float
                        wandb.run.summary['best_success_rate'] = float(success_rate)  # ç¡®ä¿æ˜¯Python float
                        wandb.run.summary['best_timestep'] = int(self.num_timesteps)  # ç¡®ä¿æ˜¯Python int
                except:
                    pass
            
            # å®šæœŸä¿å­˜checkpoint
            self.eval_count += 1
            if self.checkpoint_dir is not None and self.eval_count % self.checkpoint_freq == 0:
                # è®¡ç®—å½“å‰è¿­ä»£æ¬¡æ•°ï¼ˆè¿‘ä¼¼ï¼‰
                iteration = self.num_timesteps // (self.model.n_steps * self.model.n_envs)
                
                # åˆ›å»ºcheckpointæ–‡ä»¶åï¼šåŒ…å«è¿­ä»£æ¬¡æ•°ã€å¥–åŠ±ã€æˆåŠŸç‡
                self.checkpoint_count += 1
                checkpoint_name = f"ckpt_iter_{iteration:03d}_reward_{mean_reward:+.1f}_success_{success_rate:.2f}.zip"
                checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)
                
                # ä¿å­˜checkpoint
                self.model.save(checkpoint_path)
                
                if self.verbose > 0:
                    print(f"ğŸ’¾ ä¿å­˜checkpoint #{self.checkpoint_count}: {checkpoint_name}")
        
        return True


def make_env_fn(task, rank, seed=0):
    """åˆ›å»ºç¯å¢ƒçš„å·¥å‚å‡½æ•°"""
    def _init():
        # åœ¨æ¯ä¸ªå­è¿›ç¨‹ä¸­æŠ‘åˆ¶è­¦å‘Š
        import warnings
        warnings.filterwarnings('ignore', category=UserWarning, module='pygame.pkgdata')
        
        # åˆå¹¶env_configå’Œreward_config
        task_config = TASK_CONFIGS[task]
        config = task_config['env_config'].copy()
        if 'reward_config' in task_config:
            config['reward_config'] = task_config['reward_config']
        
        env = make_env(task, render_mode=None, config=config)
        env.reset(seed=seed + rank)
        return env
    return _init


def train(task, n_envs=None, device=None, continue_from=None, continue_iterations=None):
    """è®­ç»ƒæ¨¡å‹"""
    if task not in TASK_CONFIGS:
        raise ValueError(f"Unknown task: {task}. Available: {list(TASK_CONFIGS.keys())}")
    
    config = TASK_CONFIGS[task]
    train_cfg = config['training_config']
    ppo_cfg = config['ppo_config']
    
    if n_envs is None:
        n_envs = train_cfg['n_envs']
    
    # è®¡ç®—æ€»æ­¥æ•°
    if continue_iterations is not None:
        # ä½¿ç”¨æŒ‡å®šçš„ç»§ç»­è®­ç»ƒè¿­ä»£æ¬¡æ•°
        n_iterations = continue_iterations
        total_timesteps = n_iterations * ppo_cfg['n_steps'] * n_envs
    else:
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤è¿­ä»£æ¬¡æ•°
        n_iterations = train_cfg['n_iterations']
        total_timesteps = n_iterations * ppo_cfg['n_steps'] * n_envs
    
    print("=" * 70)
    print(f"è®­ç»ƒä»»åŠ¡: {task}")
    print("=" * 70)
    print(f"è¿­ä»£æ¬¡æ•°: {n_iterations}")
    print(f"å¹¶è¡Œç¯å¢ƒ: {n_envs}")
    print(f"æ€»æ­¥æ•°: {total_timesteps:,}")
    print("=" * 70)
    
    # è®¾å¤‡é€‰æ‹©
    if device is None:
        device = 'cpu'
    elif device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    if device == 'cpu':
        print("æç¤º: MLPç­–ç•¥åœ¨CPUä¸Šè®­ç»ƒé€šå¸¸æ›´é«˜æ•ˆ")
        if torch.cuda.is_available():
            print(f"æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}ï¼ˆæœªä½¿ç”¨ï¼‰")
            print("å¦‚éœ€ä½¿ç”¨GPUï¼Œè¯·æ·»åŠ å‚æ•°: --device cuda")
    elif device == 'cuda':
        if torch.cuda.is_available():
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print("âš ï¸  æ³¨æ„: MLPç­–ç•¥åœ¨GPUä¸Šå¯èƒ½æ¯”CPUæ…¢")
        else:
            print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPU")
            device = 'cpu'
    
    print()
    
    # åˆ›å»ºè¿è¡Œç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_name = f"{task}_{timestamp}"
    run_dir = os.path.join("runs", run_name)
    os.makedirs(run_dir, exist_ok=True)
    
    print(f"è¿è¡Œç›®å½•: {run_dir}/\n")
    
    # åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
    env = SubprocVecEnv([make_env_fn(task, i) for i in range(n_envs)])
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    task_config = TASK_CONFIGS[task]
    eval_config = task_config['env_config'].copy()
    if 'reward_config' in task_config:
        eval_config['reward_config'] = task_config['reward_config']
    eval_env = make_env(task, render_mode=None, config=eval_config)
    
    # ä¿å­˜é…ç½®
    config_save = {
        'run_name': run_name,
        'timestamp': timestamp,
        'task': task,
        'n_envs': n_envs,
        'device': device,
        'env_config': config['env_config'],
        'ppo_config': {k: str(v) if not isinstance(v, (int, float, bool, type(None))) else v 
                       for k, v in ppo_cfg.items()},
        'training_config': train_cfg,
        'network_config': config['network_config'],
    }
    
    if 'reward_config' in config:
        config_save['reward_config'] = config['reward_config']
    
    with open(os.path.join(run_dir, 'config.json'), 'w', encoding='utf-8') as f:
        json.dump(config_save, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºæˆ–åŠ è½½æ¨¡å‹
    tensorboard_log = os.path.join(run_dir, "tensorboard")
    
    if continue_from is not None:
        # ç»§ç»­è®­ç»ƒï¼šåŠ è½½å·²æœ‰æ¨¡å‹
        if not os.path.exists(continue_from):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {continue_from}")
        
        print(f"ğŸ”„ ä»å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒ: {continue_from}")
        model = PPO.load(continue_from, env=env, device=device)
        
        # æ›´æ–°tensorboardæ—¥å¿—è·¯å¾„
        model.tensorboard_log = tensorboard_log
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå°†ç»§ç»­è®­ç»ƒ {n_iterations} æ¬¡è¿­ä»£ ({total_timesteps:,} æ­¥)")
        
    else:
        # æ–°è®­ç»ƒï¼šåˆ›å»ºæ–°æ¨¡å‹
        print(f"ğŸ†• åˆ›å»ºæ–°æ¨¡å‹ï¼Œå¼€å§‹è®­ç»ƒ {n_iterations} æ¬¡è¿­ä»£ ({total_timesteps:,} æ­¥)")
        model = PPO(
            'MlpPolicy',
            env,
            verbose=1,
            learning_rate=ppo_cfg['learning_rate'],
            n_steps=ppo_cfg['n_steps'],
            batch_size=ppo_cfg['batch_size'],
            n_epochs=ppo_cfg['n_epochs'],
            gamma=ppo_cfg['gamma'],
            ent_coef=ppo_cfg['ent_coef'],
            policy_kwargs={'net_arch': config['network_config']['net_arch']},
            tensorboard_log=tensorboard_log,
            device=device
        )
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    best_model_dir = os.path.join(run_dir, "best_model")
    os.makedirs(best_model_dir, exist_ok=True)
    checkpoint_dir = os.path.join(run_dir, "checkpoints")
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # åˆ›å»ºå›è°ƒ
    reward_logger = RewardComponentLogger(log_freq=100, verbose=0)
    performance_callback = PerformanceCallback(
        eval_env=eval_env,
        eval_freq=ppo_cfg['n_steps'] * n_envs,
        n_eval_episodes=train_cfg.get('n_eval_episodes', 10),
        verbose=1,
        best_model_save_path=best_model_dir,
        checkpoint_dir=checkpoint_dir,
        checkpoint_freq=train_cfg.get('checkpoint_freq', 10)
    )
    callbacks = CallbackList([reward_logger, performance_callback])
    
    # è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒ...")
    print(f"PPOè¿­ä»£æ¬¡æ•°: {n_iterations}")
    print(f"æ€»ç¯å¢ƒæ­¥æ•°: {total_timesteps:,}")
    print(f"Checkpointé¢‘ç‡: æ¯{train_cfg.get('checkpoint_freq', 10)}æ¬¡è¯„ä¼°\n")
    
    model.learn(total_timesteps=total_timesteps, progress_bar=True, callback=callbacks)
    
    # ä¿å­˜æ¨¡å‹
    model_path = os.path.join(run_dir, "model")
    model.save(model_path)
    
    # ä¿å­˜æ‘˜è¦
    best_model_path = os.path.join(best_model_dir, "best_model")
    summary = {
        'run_name': run_name,
        'task': task,
        'total_timesteps': total_timesteps,
        'best_reward': float(performance_callback.best_mean_reward),  # è½¬æ¢ä¸ºPython float
        'checkpoint_count': performance_callback.checkpoint_count,
    }
    with open(os.path.join(run_dir, 'summary.json'), 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºREADME
    readme = f"""# è®­ç»ƒè¿è¡Œ: {run_name}

## åŸºæœ¬ä¿¡æ¯
- ä»»åŠ¡: {task}
- è®­ç»ƒæ­¥æ•°: {total_timesteps:,}
- è®¾å¤‡: {device}

## ä½¿ç”¨
```bash
python play.py --task {task} --model {best_model_path}
```
"""
    with open(os.path.join(run_dir, 'README.md'), 'w', encoding='utf-8') as f:
        f.write(readme)
    
    print("\n" + "=" * 70)
    print("è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)
    print(f"è¿è¡Œç›®å½•: {run_dir}/")
    print(f"æœ€ä½³å¥–åŠ±: {performance_callback.best_mean_reward:.2f}")
    print(f"Checkpointæ•°é‡: {performance_callback.checkpoint_count}")
    print("=" * 70)
    
    env.close()
    eval_env.close()
    return model, run_dir


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='å¤šä»»åŠ¡RLè®­ç»ƒ')
    parser.add_argument('--task', type=str, required=True,
                       choices=list(TASK_CONFIGS.keys()),
                       help='ä»»åŠ¡åç§°')
    parser.add_argument('--envs', type=int, default=None,
                       help='å¹¶è¡Œç¯å¢ƒæ•°é‡')
    parser.add_argument('--device', type=str, default=None,
                       choices=['cpu', 'cuda', 'auto'],
                       help='è®­ç»ƒè®¾å¤‡')
    parser.add_argument('--play', action='store_true',
                       help='è®­ç»ƒåç«‹å³æ¼”ç¤º')
    parser.add_argument('--continue-from', type=str, default=None,
                       help='ä»æŒ‡å®šæ¨¡å‹ç»§ç»­è®­ç»ƒ (æ¨¡å‹è·¯å¾„ï¼Œå¦‚: runs/xxx/best_model/best_model.zip)')
    parser.add_argument('--continue-iterations', type=int, default=None,
                       help='ç»§ç»­è®­ç»ƒçš„è¿­ä»£æ¬¡æ•° (é»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®)')
    
    args = parser.parse_args()
    
    # éªŒè¯ç»§ç»­è®­ç»ƒå‚æ•°
    if args.continue_from is not None:
        if not os.path.exists(args.continue_from):
            print(f"âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.continue_from}")
            sys.exit(1)
        print(f"ğŸ”„ ç»§ç»­è®­ç»ƒæ¨¡å¼")
        print(f"   æ¨¡å‹è·¯å¾„: {args.continue_from}")
        if args.continue_iterations:
            print(f"   è®­ç»ƒè¿­ä»£: {args.continue_iterations} æ¬¡")
        else:
            print(f"   è®­ç»ƒè¿­ä»£: ä½¿ç”¨é…ç½®æ–‡ä»¶é»˜è®¤å€¼")
    
    model, run_dir = train(args.task, args.envs, args.device, args.continue_from, args.continue_iterations)
    
    if args.play:
        print("\nå¯åŠ¨æ¼”ç¤º...")
        best_model = os.path.join(run_dir, "best_model", "best_model")
        if os.path.exists(f"{best_model}.zip"):
            os.system(f"python play.py --task {args.task} --model {best_model}")
