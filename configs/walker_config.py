"""
火柴人行走任务配置

任务：2D火柴人学会向右行走，保持平衡
"""

WALKER_CONFIG = {
    # ========================================================================
    # 环境配置
    # ========================================================================
    'env_config': {
        'dt': 1.0/60.0,              # 物理时间步长（秒）
        'gravity': 9.81,             # 重力加速度（m/s²）
        'max_steps': 1000,           # 每回合最大步数
        'max_torque': 100.0,         # 关节最大扭矩（N·m）
        'disable_termination': True,  # 阶段1：禁用终止条件，让火柴人探索整个episode
        
        # 火柴人参数（可选，环境有默认值）
        # 'torso_length': 0.6,       # 躯干长度（米）
        # 'thigh_length': 0.4,       # 大腿长度（米）
        # 'calf_length': 0.4,        # 小腿长度（米）
    },
    
    # ========================================================================
    # PPO算法配置
    # ========================================================================
    'ppo_config': {
        'learning_rate': 3e-4,       # 学习率
        'n_steps': 2048,             # 每次更新前收集的步数
        'batch_size': 64,            # 批量大小
        'n_epochs': 10,              # 每次数据的训练轮数
        'gamma': 0.99,               # 折扣因子
        'gae_lambda': 0.95,          # GAE lambda
        'clip_range': 0.2,           # PPO裁剪范围
        'ent_coef': 0.01,            # 熵系数（探索）
        'vf_coef': 0.5,              # 价值函数系数
        'max_grad_norm': 0.5,        # 梯度裁剪
    },
    
    # ========================================================================
    # 训练流程配置
    # ========================================================================
    'training_config': {
        'n_envs': 16,                # 并行环境数量
        'n_iterations': 200,         # 总迭代次数
        'eval_freq': 20,             # 评估频率（每N次迭代）
        'n_eval_episodes': 10,       # 每次评估的回合数
        'checkpoint_freq': 10,       # 每10次迭代保存一次checkpoint
    },
    
    # ========================================================================
    # 网络架构配置
    # ========================================================================
    'network_config': {
        'net_arch': [256, 256],      # 隐藏层大小（火柴人需要更大的网络）
        'activation_fn': 'tanh',     # 激活函数
    },
    
    # ========================================================================
    # 奖励函数配置（在环境中定义）
    # ========================================================================
    'reward_config': {
        # 新的奖励函数设计（参考BipedalWalker和相关研究）
        # 主要组件：
        # 1. velocity_reward: 鼓励以目标速度前进（高斯奖励，权重2.0）
        # 2. action_penalty: 惩罚能量消耗（权重0.001）
        # 3. upright_reward: 保持躯干直立（权重0.5）
        # 4. height_reward: 保持合适的站立高度（权重0.3）
        # 5. vertical_velocity_penalty: 减少跳跃（权重0.1）
        # 6. joint_penalty: 避免不自然的姿态（权重0.1）
        # 7. angular_velocity_penalty: 鼓励平滑运动（权重0.05）
        # 8. foot_contact_reward: 鼓励脚接触地面（权重0.2）
        
        'target_velocity': 1.5,      # 目标前进速度 (m/s)
        'velocity_weight': 2.0,      # 速度奖励权重
        'action_weight': 0.001,      # 动作惩罚权重
        'upright_weight': 0.5,       # 直立奖励权重
        'height_weight': 0.3,        # 高度奖励权重
    }
}


# ============================================================================
# 配置说明
# ============================================================================
"""
快速调整指南：

【课程学习策略 - 推荐】
阶段1：探索前进（0-100次迭代）
   - disable_termination: True  # 禁用终止条件
   - max_steps: 1000
   - 目标：让火柴人学会向前移动，不管姿态

阶段2：改进姿态（100-300次迭代）
   - disable_termination: False  # 启用终止条件
   - max_steps: 1000
   - 加载阶段1的模型继续训练
   - 目标：在保持前进的同时改进姿态

阶段3：精细调优（300+次迭代）
   - 调整奖励权重，优化步态
   - 可以增加max_steps到2000

1. 加快训练（牺牲性能）：
   - n_envs: 16 → 32
   - n_iterations: 200 → 100
   - n_steps: 2048 → 1024
   - net_arch: [256, 256] → [128, 128]

2. 提高性能（训练更慢）：
   - learning_rate: 3e-4 → 1e-4
   - n_iterations: 200 → 500
   - n_steps: 2048 → 4096
   - net_arch: [256, 256] → [512, 512]
   - ent_coef: 0.01 → 0.005

3. 更稳定的训练：
   - learning_rate: 3e-4 → 1e-4
   - ent_coef: 0.01 → 0.005
   - n_steps: 2048 → 4096

4. 更长的回合（走更远）：
   - max_steps: 1000 → 2000

5. 更强的控制力：
   - max_torque: 100.0 → 150.0

6. 调整奖励平衡：
   - 更重视前进：在_compute_reward中调整velocity_weight
   - 更重视平衡：调整upright_weight
   - 更平滑的动作：调整action_weight

训练时间估算（16环境）：
- 100次迭代：~10分钟（阶段1：探索）
- 200次迭代：~20分钟（阶段1完成）
- 500次迭代：~50分钟（阶段2：改进姿态）
"""
